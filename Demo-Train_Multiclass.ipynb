{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Python\\ImageSegmentation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "from natsort import natsorted\n",
    "from Config import PERIG_CLASSES, MODERN_CLASSES_V2\n",
    "from huggingface_hub import snapshot_download\n",
    "from Source.Utils import create_dir, show_sample_pair\n",
    "from Source.Trainer import MultiSegmentationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "layout_dataset = \"BDRC/LayoutSegmentation_Dataset\"\n",
    "\n",
    "dataset_path = snapshot_download(\n",
    "            repo_id=f\"{layout_dataset}\",\n",
    "            repo_type=\"dataset\",\n",
    "            cache_dir=\"Datasets\")\n",
    "\n",
    "with ZipFile(f\"{dataset_path}/data.zip\", 'r') as zip:\n",
    "    zip.extractall(f\"{dataset_path}\")\n",
    "\n",
    "print(f\"downloaded and extracted the dataset to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data => Images: 16914, Masks: 16914\n",
      "Validation data => Images: 16126, Masks: 16126\n",
      "Test data => Images: 16130, Masks: 16130\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"Datasets/WesternTiledDataset\"\n",
    "\n",
    "train_data = os.path.join(dataset_path, \"train\")\n",
    "val_data = os.path.join(dataset_path, \"val\")\n",
    "test_data = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "train_x = natsorted(glob(f\"{train_data}/images/*.png\"))\n",
    "train_y = natsorted(glob(f\"{train_data}/masks/*.png\"))\n",
    "\n",
    "valid_x = natsorted(glob(f\"{val_data}/images/*.png\"))\n",
    "valid_y = natsorted(glob(f\"{val_data}/masks/*.png\"))\n",
    "\n",
    "test_x = natsorted(glob(f\"{test_data}/images/*.png\"))\n",
    "test_y = natsorted(glob(f\"{test_data}/masks/*.png\"))\n",
    "\n",
    "print(f\"Training data => Images: {len(train_x)}, Masks: {len(train_y)}\")\n",
    "print(f\"Validation data => Images: {len(valid_x)}, Masks: {len(valid_y)}\")\n",
    "print(f\"Test data => Images: {len(test_x)}, Masks: {len(test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 512\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Mutliclass Segmentation trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Python\\ImageSegmentation\\.venv\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: DiceScore metric currently defaults to `average=micro`, but will change to`average=macro` in the v1.9 release. If you've explicitly set this parameter, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(dataset_path, \"Output\")\n",
    "create_dir(output_dir)\n",
    "\n",
    "segmentation_trainer = MultiSegmentationTrainer(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    valid_x,\n",
    "    valid_y,\n",
    "    test_x,\n",
    "    test_y,\n",
    "    image_width=patch_size,\n",
    "    image_height=patch_size,\n",
    "    batch_size=batch_size,\n",
    "    network=\"deeplab\",\n",
    "    output_path=output_dir,\n",
    "    classes=MODERN_CLASSES_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = next(iter(segmentation_trainer.train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "segmentation_trainer.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_trainer.export2onnx(segmentation_trainer.model, model_name=\"modernbookformat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
